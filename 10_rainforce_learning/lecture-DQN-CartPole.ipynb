{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lecture-DQN-CartPole.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"U97jWr8TaHIw","colab_type":"text"},"cell_type":"markdown","source":["# カートポール問題を多層ニューラルネットワークで解く"]},{"metadata":{"id":"zqzU8pe4aHIz","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install gym"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XBYNm-OjdRCh","colab_type":"code","colab":{}},"cell_type":"code","source":["import gym\n","import tensorflow as tf\n","import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_0xxlyx-aHI9","colab_type":"text"},"cell_type":"markdown","source":["## 環境の読み込み"]},{"metadata":{"id":"xVkrUBaSaHJD","colab_type":"code","colab":{}},"cell_type":"code","source":["env = gym.make('CartPole-v0')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"n5lXmCtDaHJU","colab_type":"text"},"cell_type":"markdown","source":["## ネットワークのクラス定義\n","\n","クラスとは、命令や変数をまとめたデータのかたまりです。\n","\n","１つのクラスオブジェクトに複数のデータを格納し、操作をすることが可能です。"]},{"metadata":{"id":"_h2hyZpfaHJV","colab_type":"code","colab":{}},"cell_type":"code","source":["class QNetwork:\n","    def __init__(self,learning_rate=0.01,state_size=4,action_size=2,hidden_size=10,name='QNetwork'):\n","        with tf.variable_scope(name):\n","            self.inputs_ = tf.placeholder(tf.float32,[None,state_size], name='inputs')\n","            self.actions_ = tf.placeholder(tf.int32,[None],name='actions')\n","            one_hot_actions = tf.one_hot(self.actions_,action_size)\n","            self.targetQs_ = tf.placeholder(tf.float32,[None],name='target')\n","            self.fc1 = tf.contrib.layers.fully_connected(self.inputs_,hidden_size)\n","            self.fc2 = tf.contrib.layers.fully_connected(self.fc1,hidden_size)\n","            self.output = tf.contrib.layers.fully_connected(self.fc2,action_size,activation_fn=None)\n","            self.Q = tf.reduce_sum(tf.multiply(self.output,one_hot_actions),axis=1)\n","            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n","            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K6TAUmuTaHJZ","colab_type":"text"},"cell_type":"markdown","source":["## エクスペリエンス・メモリの定義"]},{"metadata":{"id":"prMrDIPwaHJa","colab_type":"code","colab":{}},"cell_type":"code","source":["from collections import deque\n","class Memory():\n","    def __init__(self, max_size = 1000):\n","        self.buffer = deque(maxlen=max_size)\n","        \n","    def add(self, experience):\n","        self.buffer.append(experience)\n","        \n","    def sample(self, batch_size):\n","        idx = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n","        return [self.buffer[ii] for ii in idx]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lLMaKEgcaHJd","colab_type":"text"},"cell_type":"markdown","source":["## ハイパーパラメーターの定義と初期化"]},{"metadata":{"id":"-vB0nUMEaHJe","colab_type":"code","colab":{}},"cell_type":"code","source":["train_episodes = 1000\n","max_step = 200\n","gamma = 0.99\n","\n","explore_start = 1.0\n","explore_stop = 0.01\n","decay_rate = 0.0001\n","\n","hidden_size = 64\n","learning_rate = 0.0001\n","\n","memory_size = 10000\n","batch_size = 20\n","pretrain_length = batch_size"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gpD2eKewaHJk","colab_type":"code","colab":{}},"cell_type":"code","source":["tf.reset_default_graph()\n","mainQN = QNetwork(name='main', hidden_size=hidden_size, learning_rate=learning_rate)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QoTGLVfPaHJo","colab_type":"text"},"cell_type":"markdown","source":["## エクスペリエンスメモリーを埋めよう"]},{"metadata":{"id":"CxO9cqj5aHJq","colab_type":"code","colab":{}},"cell_type":"code","source":["env.reset()\n","\n","state,reward,done, _ = env.step(env.action_space.sample())\n","\n","memory = Memory(max_size=memory_size)\n","\n","for ii in range(pretrain_length):\n","    \n","    action = env.action_space.sample()\n","    next_state, reward, done, _ = env.step(action)\n","    \n","    if done:\n","        next_state = np.zeros(state.shape)\n","        memory.add((state,action,reward,next_state))\n","        \n","        env.reset()\n","        \n","        state, reward, done, _ = env.step(env.action_space.sample())\n","    else:\n","        memory.add((state, action, reward, next_state))\n","        state = next_state"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TRwcnRxPaHJv","colab_type":"text"},"cell_type":"markdown","source":["## トレーニング"]},{"metadata":{"id":"kA3_WwvNaHJw","colab_type":"code","colab":{}},"cell_type":"code","source":["saver = tf.train.Saver()\n","rewards_list = []\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    \n","    step = 0\n","    for ep in range(1, train_episodes):\n","        total_rewards = 0\n","        t = 0\n","        while t < max_step:\n","            step += 1\n","            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate*step)\n","            if explore_p > np.random.rand():\n","                action = env.action_space.sample()\n","            else:\n","                feed = {mainQN.inputs_: state.reshape((1,*state.shape))}\n","                Qs = sess.run(mainQN.output, feed_dict=feed)\n","                action = np.argmax(Qs)\n","                \n","            next_state, reward, done, _ = env.step(action)\n","            total_rewards += reward\n","            \n","            if done:\n","                next_state = np.zeros(state.shape)\n","                t = max_step\n","                \n","                # if ep % 100 == 0:\n","                print('Episode: {}'.format(ep),'Total Reward: {}'.format(total_rewards),\n","                     'Total Loss: {:.4f}'.format(loss), 'Explore Prob: {:.4f}'.format(explore_p))\n","                    \n","                rewards_list.append((ep, total_rewards))\n","                memory.add((state, action, reward, next_state))\n","                env.reset()\n","                state, reward, done, _ = env.step(env.action_space.sample())\n","            \n","            else:\n","                memory.add((state, action, reward, next_state))\n","                state = next_state\n","                t += 1\n","                \n","            batch = memory.sample(batch_size)\n","            # print(batch)\n","            states = np.array([each[0] for each in batch])\n","            actions = np.array([each[1] for each in batch])\n","            rewards = np.array([each[2] for each in batch])\n","            next_states = np.array([each[3] for each in batch])\n","            \n","            target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n","            episode_ends = (next_states == np.zeros(state[0].shape)).all(axis=1)\n","            target_Qs[episode_ends] = (0, 0)\n","            \n","            targets = rewards + gamma * np.max(target_Qs, axis=1)\n","            \n","            loss, _ = sess.run([mainQN.loss, mainQN.opt], \n","                              feed_dict={mainQN.inputs_: states,\n","                                        mainQN.targetQs_: targets,\n","                                        mainQN.actions_: actions})\n","            \n","    saver.save(sess, \"checkpoints/cartpole_dqn.ckpt\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"evS6Bs0NaHJ3","colab_type":"text"},"cell_type":"markdown","source":["## トレーニングの結果を可視化"]},{"metadata":{"id":"YRn3-qIiaHJ4","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","def running_mean(x, N):\n","    cumsum = np.cumsum(np.insert(x,0,0))\n","    return (cumsum[N:] - cumsum[:-N]) / N"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wjcWYOM-aHJ8","colab_type":"code","colab":{}},"cell_type":"code","source":["eps, rews = np.array(rewards_list).T\n","smoothed_rews = running_mean(rews,10)\n","plt.plot(eps[-len(smoothed_rews):],smoothed_rews)\n","plt.plot(eps, rews, color='grey', alpha=0.3)\n","plt.xlabel('Episode')\n","plt.ylabel('Total Rewards')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9mxUAd89aHKB","colab_type":"code","colab":{}},"cell_type":"code","source":["test_episodes = 10\n","test_max_steps = 400\n","env.reset()\n","with tf.Session() as sess:\n","    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n","    \n","    for ep in range(1, test_episodes):\n","        t = 0\n","        while t < test_max_steps:\n","            env.render()\n","            \n","            feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n","            Qs = sess.run(mainQN.output, feed_dict=feed)\n","            action = np.argmax(Qs)\n","            \n","            next_state, reward, done, _ = env.step(action)\n","            \n","            if done:\n","                t = test_max_steps\n","                env.reset()\n","                state, reward, done, _ = env.step(env.action_space.sample())\n","                \n","            else:\n","                state = next_state\n","                t += 1"],"execution_count":0,"outputs":[]}]}