{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lecture_torchvision_finetuning_instance_segmentation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPvEN0Vx3W02dtzNdc/T4qn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"cRL29lVh4sCY","colab_type":"code","colab":{}},"source":["%%shell\n","\n","# pycocotoolsをインストール\n","git clone https://github.com/cocodataset/cocoapi.git\n","cd cocoapi/PythonAPI\n","python setup.py build_ext install"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fVsmAgvd4yP2","colab_type":"code","colab":{}},"source":["%%shell\n","\n","# Penn-Fudan datasetをダウンロード\n","wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .\n","# 現在のフォルダに解凍\n","unzip PennFudanPed.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b6g49LBq41fq","colab_type":"code","colab":{}},"source":["from PIL import Image\n","Image.open('PennFudanPed/PNGImages/FudanPed00001.png')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ruTCZOzT44Wy","colab_type":"code","colab":{}},"source":["mask = Image.open('PennFudanPed/PedMasks/FudanPed00001_mask.png')\n","# 各マスクインスタンスには、0からNまでの異なる色がある。\n","# Nはインスタンスの数です。視覚化を簡単にするために、マスクにカラーパレットを追加する。\n","mask.putpalette([\n","    0, 0, 0, # black background\n","    255, 0, 0, # index 1 is red\n","    255, 255, 0, # index 2 is yellow\n","    255, 153, 0, # index 3 is orange\n","])\n","mask"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s69MIOek452F","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","import torch\n","import torch.utils.data\n","from PIL import Image\n","\n","\n","class PennFudanDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, transforms=None):\n","        self.root = root\n","        self.transforms = transforms\n","        # すべての画像ファイルをロードし、並べ替えて整列させる\n","        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n","        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n","\n","    def __getitem__(self, idx):\n","        # load images ad masks\n","        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n","        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n","        img = Image.open(img_path).convert(\"RGB\")\n","        # 各色は異なるインスタンスに対応し、0が背景であるため、マスクをRGBに変換していないことに注意\n","        mask = Image.open(mask_path)\n","\n","        mask = np.array(mask)\n","        # インスタンスは異なる色としてエンコードされる\n","        obj_ids = np.unique(mask)\n","        # 最初のIDは背景なので、削除\n","        obj_ids = obj_ids[1:]\n","\n","        # カラーエンコードされたマスクをバイナリマスクのセットに分割\n","        masks = mask == obj_ids[:, None, None]\n","\n","        # 各マスクの境界ボックス座標を取得\n","        num_objs = len(obj_ids)\n","        boxes = []\n","        for i in range(num_objs):\n","            pos = np.where(masks[i])\n","            xmin = np.min(pos[1])\n","            xmax = np.max(pos[1])\n","            ymin = np.min(pos[0])\n","            ymax = np.max(pos[0])\n","            boxes.append([xmin, ymin, xmax, ymax])\n","\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        # クラスは1つだけ\n","        labels = torch.ones((num_objs,), dtype=torch.int64)\n","        masks = torch.as_tensor(masks, dtype=torch.uint8)\n","\n","        image_id = torch.tensor([idx])\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        # すべてのインスタンスが混雑していないとする\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"masks\"] = masks\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            img, target = self.transforms(img, target)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.imgs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rqLNWTvR49Z7","colab_type":"code","colab":{}},"source":["dataset = PennFudanDataset('PennFudanPed/')\n","dataset[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3dGueqOT4_TM","colab_type":"code","colab":{}},"source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","      \n","def get_instance_segmentation_model(num_classes):\n","    # COCOで事前トレーニングされたインスタンスセグメンテーションモデルをロードする\n","    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n","\n","    # 分類器の入力特徴量の数を取得する\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # 事前に訓練されたヘッドを新しいヘッドと交換する\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    # マスク分類器の入力特徴量の数を取得する\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","    # マスク予測器を新しいものに置き換える\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n","                                                       hidden_layer,\n","                                                       num_classes)\n","\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j-J_Eik85Efq","colab_type":"code","colab":{}},"source":["%%shell\n","\n","# TorchVisionリポジトリをダウンロード\n","git clone https://github.com/pytorch/vision.git\n","cd vision\n","git checkout v0.3.0\n","\n","cp references/detection/utils.py ../\n","cp references/detection/transforms.py ../\n","cp references/detection/coco_eval.py ../\n","cp references/detection/engine.py ../\n","cp references/detection/coco_utils.py ../"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b_OXnYDR5GSq","colab_type":"code","colab":{}},"source":["from engine import train_one_epoch, evaluate\n","import utils\n","import transforms as T\n","\n","def get_transform(train):\n","    transforms = []\n","    # 画像、PIL画像をPyTorchテンソルに変換\n","    transforms.append(T.ToTensor())\n","    if train:\n","        # トレーニング中、データ増強のためにトレーニング画像とグラウンドトゥルースをランダムに反転\n","        transforms.append(T.RandomHorizontalFlip(0.5))\n","    return T.Compose(transforms)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PIPwrE765IZX","colab_type":"code","colab":{}},"source":["# データセットとtransformを使用\n","dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n","dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n","\n","# 訓練とテストセットでデータセットを分割する\n","torch.manual_seed(1)\n","indices = torch.randperm(len(dataset)).tolist()\n","dataset = torch.utils.data.Subset(dataset, indices[:-50])\n","dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n","\n","# 訓練および検証データローダーを定義する\n","data_loader = torch.utils.data.DataLoader(\n","    dataset, batch_size=2, shuffle=True, num_workers=4,\n","    collate_fn=utils.collate_fn)\n","\n","data_loader_test = torch.utils.data.DataLoader(\n","    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n","    collate_fn=utils.collate_fn)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sTYPBMGd5KgX","colab_type":"code","colab":{}},"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# データセットには、背景と個人の2つのクラスのみがある\n","num_classes = 2\n","\n","# ヘルパー関数を使用してモデルを取得\n","model = get_instance_segmentation_model(num_classes)\n","# モデルを適切なデバイスに移動\n","model.to(device)\n","\n","# optimizerを構築\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005,\n","                            momentum=0.9, weight_decay=0.0005)\n","\n","# 3エポックごとに10倍ずつ学習率を下げる学習率スケジューラー\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                               step_size=3,\n","                                               gamma=0.1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8KwDBDAK5MQT","colab_type":"code","colab":{}},"source":["# 10 epochsで訓練\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    # 1エポックでトレーニングし、10回の反復ごとに表示\n","    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n","    # 学習率を更新\n","    lr_scheduler.step()\n","    # テストデータセットで評価\n","    evaluate(model, data_loader_test, device=device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yZ0Wenzg5ORx","colab_type":"code","colab":{}},"source":["# テストセットから画像を1つ選択する\n","img, _ = dataset_test[0]\n","# モデルを評価モードにする\n","model.eval()\n","with torch.no_grad():\n","    prediction = model([img.to(device)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1T_cdXUi5QoZ","colab_type":"code","colab":{}},"source":["prediction"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LO6uzpe75R9Z","colab_type":"code","colab":{}},"source":["Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ftimr6005TYW","colab_type":"code","colab":{}},"source":["Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zTnLkbzJ8nNi","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}