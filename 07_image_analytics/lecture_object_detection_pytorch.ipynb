{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lecture_object_detection_pytorch.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNWg3TOb7ALUgzUJLhkD7zf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"opstuleeiMog","colab_type":"text"},"source":["# Dataload"]},{"cell_type":"code","metadata":{"id":"wmlUTKT12S9s","colab_type":"code","colab":{}},"source":["import os\n","import urllib.request\n","import zipfile\n","import tarfile"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b41OdFPb2TAE","colab_type":"code","colab":{}},"source":["# フォルダ「data」が存在しない場合は作成する\n","data_dir = \"./data/\"\n","if not os.path.exists(data_dir):\n","    os.mkdir(data_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xo8odY5d2TCx","colab_type":"code","colab":{}},"source":["# フォルダ「weights」が存在しない場合は作成する\n","weights_dir = \"./weights/\"\n","if not os.path.exists(weights_dir):\n","    os.mkdir(weights_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iGRhxLee2TFK","colab_type":"code","colab":{}},"source":["# VOC2012のデータセットをダウンロードする\n","url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\"\n","target_path = os.path.join(data_dir, \"VOCtrainval_11-May-2012.tar\") \n","\n","if not os.path.exists(target_path):\n","    urllib.request.urlretrieve(url, target_path)\n","    \n","    tar = tarfile.TarFile(target_path)  # tarファイルを読み込み\n","    tar.extractall(data_dir)  # tarを解凍\n","    tar.close()  # tarファイルをクローズ"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KcpsdzRO2cav","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"e4a45952-e027-474a-c74c-ba4d5de4620a","executionInfo":{"status":"ok","timestamp":1581942361913,"user_tz":-540,"elapsed":196866,"user":{"displayName":"nori 86","photoUrl":"","userId":"17990641330801160498"}}},"source":["# 学習済みのSSD用のVGGのパラメータをフォルダ「weights」にダウンロード\n","    \n","url = \"https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth\"\n","target_path = os.path.join(weights_dir, \"vgg16_reducedfc.pth\") \n","\n","urllib.request.urlretrieve(url, target_path)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('./weights/vgg16_reducedfc.pth', <http.client.HTTPMessage at 0x7f34bb629080>)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"yq5iylUE2cdT","colab_type":"code","colab":{}},"source":["# 学習済みのSSD300モデルをフォルダ「weights」にダウンロード\n","\n","url = \"https://s3.amazonaws.com/amdegroot-models/ssd300_mAP_77.43_v2.pth\"\n","target_path = os.path.join(weights_dir, \"ssd300_mAP_77.43_v2.pth\") \n","\n","if not os.path.exists(target_path):\n","    urllib.request.urlretrieve(url, target_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"trUNJgLp1DB8","colab_type":"code","colab":{}},"source":["# パッケージのimport\n","import os.path as osp\n","import random\n","# XMLをファイルやテキストから読み込んだり、加工したり、保存したりするためのライブラリ\n","import xml.etree.ElementTree as ET\n","\n","import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.utils.data as data\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dEOY2KRy10P5","colab_type":"code","colab":{}},"source":["# 乱数のシードを設定\n","torch.manual_seed(1234)\n","np.random.seed(1234)\n","random.seed(1234)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"huI2rycj12HZ","colab_type":"code","colab":{}},"source":["# 学習、検証の画像データとアノテーションデータへのファイルパスリストを作成する\n","\n","def make_datapath_list(rootpath):\n","\n","    # 画像ファイルとアノテーションファイルへのパスのテンプレートを作成\n","    imgpath_template = osp.join(rootpath, 'JPEGImages', '%s.jpg')\n","    annopath_template = osp.join(rootpath, 'Annotations', '%s.xml')\n","\n","    # 訓練と検証、それぞれのファイルのID（ファイル名）を取得する\n","    train_id_names = osp.join(rootpath + 'ImageSets/Main/train.txt')\n","    val_id_names = osp.join(rootpath + 'ImageSets/Main/val.txt')\n","\n","    # 訓練データの画像ファイルとアノテーションファイルへのパスリストを作成\n","    train_img_list = list()\n","    train_anno_list = list()\n","\n","    for line in open(train_id_names):\n","        file_id = line.strip()  # 空白スペースと改行を除去\n","        img_path = (imgpath_template % file_id)  # 画像のパス\n","        anno_path = (annopath_template % file_id)  # アノテーションのパス\n","        train_img_list.append(img_path)  # リストに追加\n","        train_anno_list.append(anno_path)  # リストに追加\n","\n","    # 検証データの画像ファイルとアノテーションファイルへのパスリストを作成\n","    val_img_list = list()\n","    val_anno_list = list()\n","\n","    for line in open(val_id_names):\n","        file_id = line.strip()  # 空白スペースと改行を除去\n","        img_path = (imgpath_template % file_id)  # 画像のパス\n","        anno_path = (annopath_template % file_id)  # アノテーションのパス\n","        val_img_list.append(img_path)  # リストに追加\n","        val_anno_list.append(anno_path)  # リストに追加\n","\n","    return train_img_list, train_anno_list, val_img_list, val_anno_list"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ucgLegjF15wr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"2e022002-ec51-4266-99a9-e900fb0c2dd5","executionInfo":{"status":"ok","timestamp":1581942369066,"user_tz":-540,"elapsed":203981,"user":{"displayName":"nori 86","photoUrl":"","userId":"17990641330801160498"}}},"source":["# ファイルパスのリストを作成\n","rootpath = \"./data/VOCdevkit/VOC2012/\"\n","train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\n","    rootpath)\n","\n","# 動作確認\n","print(train_img_list[0])"],"execution_count":10,"outputs":[{"output_type":"stream","text":["./data/VOCdevkit/VOC2012/JPEGImages/2008_000008.jpg\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ITZuSj3N2vSJ","colab_type":"text"},"source":["# xml形式のアノテーションデータをリストに変換する"]},{"cell_type":"code","metadata":{"id":"HY0tkN2f18B1","colab_type":"code","colab":{}},"source":["# 「XML形式のアノテーション」を、リスト形式に変換するクラス\n","\n","\n","class Anno_xml2list(object):\n","    \"\"\"\n","    1枚の画像に対する「XML形式のアノテーションデータ」を、画像サイズで規格化してからリスト形式に変換する。\n","\n","    Attributes\n","    ----------\n","    classes : リスト\n","        VOCのクラス名を格納したリスト\n","    \"\"\"\n","\n","    def __init__(self, classes):\n","\n","        self.classes = classes\n","\n","    def __call__(self, xml_path, width, height):\n","        \"\"\"\n","        1枚の画像に対する「XML形式のアノテーションデータ」を、画像サイズで規格化してからリスト形式に変換する。\n","\n","        Parameters\n","        ----------\n","        xml_path : str\n","            xmlファイルへのパス。\n","        width : int\n","            対象画像の幅。\n","        height : int\n","            対象画像の高さ。\n","\n","        Returns\n","        -------\n","        ret : [[xmin, ymin, xmax, ymax, label_ind], ... ]\n","            物体のアノテーションデータを格納したリスト。画像内に存在する物体数分のだけ要素を持つ。\n","        \"\"\"\n","\n","        # 画像内の全ての物体のアノテーションをこのリストに格納します\n","        ret = []\n","\n","        # xmlファイルを読み込む\n","        xml = ET.parse(xml_path).getroot()\n","\n","        # 画像内にある物体（object）の数だけループする\n","        for obj in xml.iter('object'):\n","\n","            # アノテーションで検知がdifficultに設定されているものは除外\n","            difficult = int(obj.find('difficult').text)\n","            if difficult == 1:\n","                continue\n","\n","            # 1つの物体に対するアノテーションを格納するリスト\n","            bndbox = []\n","\n","            name = obj.find('name').text.lower().strip()  # 物体名\n","            bbox = obj.find('bndbox')  # バウンディングボックスの情報\n","\n","            # アノテーションの xmin, ymin, xmax, ymaxを取得し、0～1に規格化\n","            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n","\n","            for pt in (pts):\n","                # VOCは原点が(1,1)なので1を引き算して（0, 0）に\n","                cur_pixel = int(bbox.find(pt).text) - 1\n","\n","                # 幅、高さで規格化\n","                if pt == 'xmin' or pt == 'xmax':  # x方向のときは幅で割算\n","                    cur_pixel /= width\n","                else:  # y方向のときは高さで割算\n","                    cur_pixel /= height\n","\n","                bndbox.append(cur_pixel)\n","\n","            # アノテーションのクラス名のindexを取得して追加\n","            label_idx = self.classes.index(name)\n","            bndbox.append(label_idx)\n","\n","            # resに[xmin, ymin, xmax, ymax, label_ind]を足す\n","            ret += [bndbox]\n","\n","        return np.array(ret)  # [[xmin, ymin, xmax, ymax, label_ind], ... ]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JTGQ3yfv223D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"ec72eff0-6001-41e8-e4c8-fbfe4d619215","executionInfo":{"status":"ok","timestamp":1581942369068,"user_tz":-540,"elapsed":203971,"user":{"displayName":"nori 86","photoUrl":"","userId":"17990641330801160498"}}},"source":["# 動作確認　\n","voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n","               'bottle', 'bus', 'car', 'cat', 'chair',\n","               'cow', 'diningtable', 'dog', 'horse',\n","               'motorbike', 'person', 'pottedplant',\n","               'sheep', 'sofa', 'train', 'tvmonitor']\n","\n","transform_anno = Anno_xml2list(voc_classes)\n","\n","# 画像の読み込み OpenCVを使用\n","ind = 1\n","image_file_path = val_img_list[ind]\n","img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n","height, width, channels = img.shape  # 画像のサイズを取得\n","\n","# アノテーションをリストで表示\n","transform_anno(val_anno_list[ind], width, height)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.09      ,  0.03003003,  0.998     ,  0.996997  , 18.        ],\n","       [ 0.122     ,  0.56756757,  0.164     ,  0.72672673, 14.        ]])"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"nvbPhyt6280e","colab_type":"text"},"source":["# 画像とアノテーションの前処理を行うクラスDataTransformを作成する"]},{"cell_type":"code","metadata":{"id":"Z6U1XuZ03p6L","colab_type":"code","colab":{}},"source":["!mkdir utils"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OUhSlcW7Pkfn","colab_type":"text"},"source":["### utilsフォルダにアップロード"]},{"cell_type":"code","metadata":{"id":"4SOAEvl225xT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":413},"outputId":"e7fee475-e446-4a5f-b203-8ab452967530","executionInfo":{"status":"error","timestamp":1581942371239,"user_tz":-540,"elapsed":206131,"user":{"displayName":"nori 86","photoUrl":"","userId":"17990641330801160498"}}},"source":["# フォルダ「utils」にあるdata_augumentation.pyからimport。\n","# 入力画像の前処理をするクラス\n","from utils.data_augumentation import Compose, ConvertFromInts, ToAbsoluteCoords, \\\n","PhotometricDistort, Expand, RandomSampleCrop, RandomMirror, ToPercentCoords, Resize, SubtractMeans\n","\n","class DataTransform():\n","    \"\"\"\n","    画像とアノテーションの前処理クラス。訓練と推論で異なる動作をする。\n","    画像のサイズを300x300にする。\n","    学習時はデータオーギュメンテーションする。\n","\n","    Attributes\n","    ----------\n","    input_size : int\n","        リサイズ先の画像の大きさ。\n","    color_mean : (B, G, R)\n","        各色チャネルの平均値。\n","    \"\"\"\n","\n","    def __init__(self, input_size, color_mean):\n","        self.data_transform = {\n","            'train': Compose([\n","                ConvertFromInts(),  # intをfloat32に変換\n","                ToAbsoluteCoords(),  # アノテーションデータの規格化を戻す\n","                PhotometricDistort(),  # 画像の色調などをランダムに変化\n","                Expand(color_mean),  # 画像のキャンバスを広げる\n","                RandomSampleCrop(),  # 画像内の部分をランダムに抜き出す\n","                RandomMirror(),  # 画像を反転させる\n","                ToPercentCoords(),  # アノテーションデータを0-1に規格化\n","                Resize(input_size),  # 画像サイズをinput_size×input_sizeに変形\n","                SubtractMeans(color_mean)  # BGRの色の平均値を引き算\n","            ]),\n","            'val': Compose([\n","                ConvertFromInts(),  # intをfloatに変換\n","                Resize(input_size),  # 画像サイズをinput_size×input_sizeに変形\n","                SubtractMeans(color_mean)  # BGRの色の平均値を引き算\n","            ])\n","        }\n","\n","    def __call__(self, img, phase, boxes, labels):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        phase : 'train' or 'val'\n","            前処理のモードを指定。\n","        \"\"\"\n","        return self.data_transform[phase](img, boxes, labels)"],"execution_count":14,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-2efc29ec3482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_augumentation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConvertFromInts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToAbsoluteCoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPhotometricDistort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExpand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomSampleCrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomMirror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToPercentCoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubtractMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDataTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0m画像とアノテーションの前処理クラス\u001b[0m\u001b[0;31m。\u001b[0m\u001b[0m訓練と推論で異なる動作をする\u001b[0m\u001b[0;31m。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.data_augumentation'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"7Qd6RqGc3Cme","colab_type":"code","colab":{}},"source":["# 動作の確認\n","\n","# 1. 画像読み込み\n","image_file_path = train_img_list[0]\n","img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n","height, width, channels = img.shape  # 画像のサイズを取得\n","\n","# 2. アノテーションをリストに\n","transform_anno = Anno_xml2list(voc_classes)\n","anno_list = transform_anno(train_anno_list[0], width, height)\n","\n","# 3. 元画像の表示\n","plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","plt.show()\n","\n","# 4. 前処理クラスの作成\n","color_mean = (104, 117, 123)  # (BGR)の色の平均値\n","input_size = 300  # 画像のinputサイズを300×300にする\n","transform = DataTransform(input_size, color_mean)\n","\n","# 5. train画像の表示\n","phase = \"train\"\n","img_transformed, boxes, labels = transform(\n","    img, phase, anno_list[:, :4], anno_list[:, 4])\n","plt.imshow(cv2.cvtColor(img_transformed, cv2.COLOR_BGR2RGB))\n","plt.show()\n","\n","# 6. val画像の表示\n","phase = \"val\"\n","img_transformed, boxes, labels = transform(\n","    img, phase, anno_list[:, :4], anno_list[:, 4])\n","plt.imshow(cv2.cvtColor(img_transformed, cv2.COLOR_BGR2RGB))\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tQFlPCvx6CoH","colab_type":"text"},"source":["# Datasetを作成する"]},{"cell_type":"code","metadata":{"id":"ExEo-O_W58b8","colab_type":"code","colab":{}},"source":["# VOC2012のDatasetを作成する\n","\n","class VOCDataset(data.Dataset):\n","    \"\"\"\n","    VOC2012のDatasetを作成するクラス。PyTorchのDatasetクラスを継承。\n","\n","    Attributes\n","    ----------\n","    img_list : リスト\n","        画像のパスを格納したリスト\n","    anno_list : リスト\n","        アノテーションへのパスを格納したリスト\n","    phase : 'train' or 'test'\n","        学習か訓練かを設定する。\n","    transform : object\n","        前処理クラスのインスタンス\n","    transform_anno : object\n","        xmlのアノテーションをリストに変換するインスタンス\n","    \"\"\"\n","\n","    def __init__(self, img_list, anno_list, phase, transform, transform_anno):\n","        self.img_list = img_list\n","        self.anno_list = anno_list\n","        self.phase = phase  # train もしくは valを指定\n","        self.transform = transform  # 画像の変形\n","        self.transform_anno = transform_anno  # アノテーションデータをxmlからリストへ\n","\n","    def __len__(self):\n","        '''画像の枚数を返す'''\n","        return len(self.img_list)\n","\n","    def __getitem__(self, index):\n","        '''\n","        前処理をした画像のテンソル形式のデータとアノテーションを取得\n","        '''\n","        im, gt, h, w = self.pull_item(index)\n","        return im, gt\n","\n","    def pull_item(self, index):\n","        '''前処理をした画像のテンソル形式のデータ、アノテーション、画像の高さ、幅を取得する'''\n","\n","        # 1. 画像読み込み\n","        image_file_path = self.img_list[index]\n","        img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n","        height, width, channels = img.shape  # 画像のサイズを取得\n","\n","        # 2. xml形式のアノテーション情報をリストに\n","        anno_file_path = self.anno_list[index]\n","        anno_list = self.transform_anno(anno_file_path, width, height)\n","\n","        # 3. 前処理を実施\n","        img, boxes, labels = self.transform(\n","            img, self.phase, anno_list[:, :4], anno_list[:, 4])\n","\n","        # 色チャネルの順番がBGRになっているので、RGBに順番変更\n","        # さらに（高さ、幅、色チャネル）の順を（色チャネル、高さ、幅）に変換\n","        img = torch.from_numpy(img[:, :, (2, 1, 0)]).permute(2, 0, 1)\n","\n","        # BBoxとラベルをセットにしたnp.arrayを作成、変数名「gt」はground truth（答え）の略称\n","        gt = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n","\n","        return img, gt, height, width"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TPtV67Eo6POQ","colab_type":"code","colab":{}},"source":["# 動作確認\n","color_mean = (104, 117, 123)  # (BGR)の色の平均値\n","input_size = 300  # 画像のinputサイズを300×300にする\n","\n","train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", transform=DataTransform(\n","    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n","\n","val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(\n","    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n","\n","\n","# データの取り出し例\n","val_dataset.__getitem__(1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1s3ANDcE6Ymd","colab_type":"text"},"source":["# DataLoaderを作成する"]},{"cell_type":"code","metadata":{"id":"PG5fGqSW6SoF","colab_type":"code","colab":{}},"source":["def od_collate_fn(batch):\n","    \"\"\"\n","    Datasetから取り出すアノテーションデータのサイズが画像ごとに異なる。\n","    画像内の物体数が2個であれば(2, 5)というサイズであるが、3個であれば（3, 5）など変化する。\n","    この変化に対応したDataLoaderを作成するために、\n","    カスタイマイズした、collate_fnを作成する。\n","    collate_fnは、PyTorchでリストからmini-batchを作成する関数。\n","    ミニバッチ分の画像が並んでいるリスト変数batchに、\n","    ミニバッチ番号を指定する次元を先頭に1つ追加して、リストの形を変形する。\n","    \"\"\"\n","\n","    targets = []\n","    imgs = []\n","    for sample in batch:\n","        imgs.append(sample[0])  # sample[0] は画像imgです\n","        targets.append(torch.FloatTensor(sample[1]))  # sample[1] はアノテーションgtです\n","\n","    # imgsはミニバッチサイズのリストになっています\n","    # リストの要素はtorch.Size([3, 300, 300])です。\n","    # このリストをtorch.Size([batch_num, 3, 300, 300])のテンソルに変換します\n","    imgs = torch.stack(imgs, dim=0)\n","\n","    # targetsはアノテーションデータの正解であるgtのリストです。\n","    # リストのサイズはミニバッチサイズです。\n","    # リストtargetsの要素は [n, 5] となっています。\n","    # nは画像ごとに異なり、画像内にある物体の数となります。\n","    # 5は [xmin, ymin, xmax, ymax, class_index] です\n","\n","    return imgs, targets"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u6qJrNZP6c6D","colab_type":"code","colab":{}},"source":["# データローダーの作成\n","batch_size = 4\n","\n","train_dataloader = data.DataLoader(\n","    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn)\n","\n","val_dataloader = data.DataLoader(\n","    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn)\n","\n","# 辞書型変数にまとめる\n","dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n","\n","# 動作の確認\n","batch_iterator = iter(dataloaders_dict[\"val\"])  # イタレータに変換\n","images, targets = next(batch_iterator)  # 1番目の要素を取り出す\n","print(images.size())  # torch.Size([4, 3, 300, 300])\n","print(len(targets))\n","print(targets[1].size())  # ミニバッチのサイズのリスト、各要素は[n, 5]、nは物体数"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZBqmiBRi6pMW","colab_type":"code","colab":{}},"source":["print(train_dataset.__len__())\n","print(val_dataset.__len__())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPrF1mB36rz2","colab_type":"code","colab":{}},"source":["# パッケージのimport\n","from math import sqrt\n","from itertools import product\n","\n","import pandas as pd\n","import torch\n","from torch.autograd import Function\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.init as init"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sqj8fC-96zL-","colab_type":"code","colab":{}},"source":["# 34層にわたる、vggモジュールを作成\n","def make_vgg():\n","    layers = []\n","    in_channels = 3  # 色チャネル数\n","\n","    # vggモジュールで使用する畳み込み層やマックスプーリングのチャネル数\n","    cfg = [64, 64, 'M', 128, 128, 'M', 256, 256,\n","           256, 'MC', 512, 512, 512, 'M', 512, 512, 512]\n","\n","    for v in cfg:\n","        if v == 'M':\n","            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","        elif v == 'MC':\n","            # ceilは出力サイズを、計算結果（float）に対して、切り上げで整数にするモード\n","            # デフォルトでは出力サイズを計算結果（float）に対して、切り下げで整数にするfloorモード\n","            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n","        else:\n","            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n","            layers += [conv2d, nn.ReLU(inplace=True)]\n","            in_channels = v\n","\n","    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n","    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n","    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n","    layers += [pool5, conv6,\n","               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n","    return nn.ModuleList(layers)\n","\n","\n","# 動作確認\n","vgg_test = make_vgg()\n","print(vgg_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nV9ePeEH7L7m","colab_type":"text"},"source":["# extrasモジュールを実装"]},{"cell_type":"code","metadata":{"id":"ttXaVxv963u0","colab_type":"code","colab":{}},"source":["# 8層にわたる、extrasモジュールを作成\n","def make_extras():\n","    layers = []\n","    in_channels = 1024  # vggモジュールから出力された、extraに入力される画像チャネル数\n","\n","    # extraモジュールの畳み込み層のチャネル数を設定するコンフィギュレーション\n","    cfg = [256, 512, 128, 256, 128, 256, 128, 256]\n","\n","    layers += [nn.Conv2d(in_channels, cfg[0], kernel_size=(1))]\n","    layers += [nn.Conv2d(cfg[0], cfg[1], kernel_size=(3), stride=2, padding=1)]\n","    layers += [nn.Conv2d(cfg[1], cfg[2], kernel_size=(1))]\n","    layers += [nn.Conv2d(cfg[2], cfg[3], kernel_size=(3), stride=2, padding=1)]\n","    layers += [nn.Conv2d(cfg[3], cfg[4], kernel_size=(1))]\n","    layers += [nn.Conv2d(cfg[4], cfg[5], kernel_size=(3))]\n","    layers += [nn.Conv2d(cfg[5], cfg[6], kernel_size=(1))]\n","    layers += [nn.Conv2d(cfg[6], cfg[7], kernel_size=(3))]\n","    \n","    # 活性化関数のReLUは今回はSSDモデルの順伝搬のなかで用意することにし、\n","    # extraモジュールでは用意していません\n","\n","    return nn.ModuleList(layers)\n","\n","\n","# 動作確認\n","extras_test = make_extras()\n","print(extras_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PeFpYlXB7Q6z","colab_type":"text"},"source":["# locモジュールとconfモジュールを実装"]},{"cell_type":"code","metadata":{"id":"nQsL4Hoi7Hnq","colab_type":"code","colab":{}},"source":["# デフォルトボックスのオフセットを出力するloc_layers、\n","# デフォルトボックスに対する各クラスの信頼度confidenceを出力するconf_layersを作成\n","\n","def make_loc_conf(num_classes=21, bbox_aspect_num=[4, 6, 6, 6, 4, 4]):\n","\n","    loc_layers = []\n","    conf_layers = []\n","\n","    # VGGの22層目、conv4_3（source1）に対する畳み込み層\n","    loc_layers += [nn.Conv2d(512, bbox_aspect_num[0]\n","                             * 4, kernel_size=3, padding=1)]\n","    conf_layers += [nn.Conv2d(512, bbox_aspect_num[0]\n","                              * num_classes, kernel_size=3, padding=1)]\n","\n","    # VGGの最終層（source2）に対する畳み込み層\n","    loc_layers += [nn.Conv2d(1024, bbox_aspect_num[1]\n","                             * 4, kernel_size=3, padding=1)]\n","    conf_layers += [nn.Conv2d(1024, bbox_aspect_num[1]\n","                              * num_classes, kernel_size=3, padding=1)]\n","\n","    # extraの（source3）に対する畳み込み層\n","    loc_layers += [nn.Conv2d(512, bbox_aspect_num[2]\n","                             * 4, kernel_size=3, padding=1)]\n","    conf_layers += [nn.Conv2d(512, bbox_aspect_num[2]\n","                              * num_classes, kernel_size=3, padding=1)]\n","\n","    # extraの（source4）に対する畳み込み層\n","    loc_layers += [nn.Conv2d(256, bbox_aspect_num[3]\n","                             * 4, kernel_size=3, padding=1)]\n","    conf_layers += [nn.Conv2d(256, bbox_aspect_num[3]\n","                              * num_classes, kernel_size=3, padding=1)]\n","\n","    # extraの（source5）に対する畳み込み層\n","    loc_layers += [nn.Conv2d(256, bbox_aspect_num[4]\n","                             * 4, kernel_size=3, padding=1)]\n","    conf_layers += [nn.Conv2d(256, bbox_aspect_num[4]\n","                              * num_classes, kernel_size=3, padding=1)]\n","\n","    # extraの（source6）に対する畳み込み層\n","    loc_layers += [nn.Conv2d(256, bbox_aspect_num[5]\n","                             * 4, kernel_size=3, padding=1)]\n","    conf_layers += [nn.Conv2d(256, bbox_aspect_num[5]\n","                              * num_classes, kernel_size=3, padding=1)]\n","\n","    return nn.ModuleList(loc_layers), nn.ModuleList(conf_layers)\n","\n","\n","# 動作確認\n","loc_test, conf_test = make_loc_conf()\n","print(loc_test)\n","print(conf_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kb_ViIGI7alB","colab_type":"text"},"source":["# L2Norm層を実装"]},{"cell_type":"code","metadata":{"id":"_9HlMBxb7X7l","colab_type":"code","colab":{}},"source":["# convC4_3からの出力をscale=20のL2Normで正規化する層\n","class L2Norm(nn.Module):\n","    def __init__(self, input_channels=512, scale=20):\n","        super(L2Norm, self).__init__()  # 親クラスのコンストラクタ実行\n","        self.weight = nn.Parameter(torch.Tensor(input_channels))\n","        self.scale = scale  # 係数weightの初期値として設定する値\n","        self.reset_parameters()  # パラメータの初期化\n","        self.eps = 1e-10\n","\n","    def reset_parameters(self):\n","        '''結合パラメータを大きさscaleの値にする初期化を実行'''\n","        init.constant_(self.weight, self.scale)  # weightの値がすべてscale（=20）になる\n","\n","    def forward(self, x):\n","        '''38×38の特徴量に対して、512チャネルにわたって2乗和のルートを求めた\n","        38×38個の値を使用し、各特徴量を正規化してから係数をかけ算する層'''\n","\n","        # 各チャネルにおける38×38個の特徴量のチャネル方向の2乗和を計算し、\n","        # さらにルートを求め、割り算して正規化する\n","        # normのテンソルサイズはtorch.Size([batch_num, 1, 38, 38])になる\n","        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()+self.eps\n","        x = torch.div(x, norm)\n","\n","        # 係数をかける。係数はチャネルごとに1つで、512個の係数を持つ\n","        # self.weightのテンソルサイズはtorch.Size([512])なので\n","        # torch.Size([batch_num, 512, 38, 38])まで変形する\n","        weights = self.weight.unsqueeze(\n","            0).unsqueeze(2).unsqueeze(3).expand_as(x)\n","        out = weights * x\n","\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ObLD1SF47hS9","colab_type":"text"},"source":["# デフォルトボックスを実装"]},{"cell_type":"code","metadata":{"id":"NxOA-G5H7e5i","colab_type":"code","colab":{}},"source":["# デフォルトボックスを出力するクラス\n","class DBox(object):\n","    def __init__(self, cfg):\n","        super(DBox, self).__init__()\n","\n","        # 初期設定\n","        self.image_size = cfg['input_size']  # 画像サイズの300\n","        # [38, 19, …] 各sourceの特徴量マップのサイズ\n","        self.feature_maps = cfg['feature_maps']\n","        self.num_priors = len(cfg[\"feature_maps\"])  # sourceの個数=6\n","        self.steps = cfg['steps']  # [8, 16, …] DBoxのピクセルサイズ\n","        self.min_sizes = cfg['min_sizes']  # [30, 60, …] 小さい正方形のDBoxのピクセルサイズ\n","        self.max_sizes = cfg['max_sizes']  # [60, 111, …] 大きい正方形のDBoxのピクセルサイズ\n","        self.aspect_ratios = cfg['aspect_ratios']  # 長方形のDBoxのアスペクト比\n","\n","    def make_dbox_list(self):\n","        '''DBoxを作成する'''\n","        mean = []\n","        # 'feature_maps': [38, 19, 10, 5, 3, 1]\n","        for k, f in enumerate(self.feature_maps):\n","            for i, j in product(range(f), repeat=2):  # fまでの数で2ペアの組み合わせを作る　f_P_2 個\n","                # 特徴量の画像サイズ\n","                # 300 / 'steps': [8, 16, 32, 64, 100, 300],\n","                f_k = self.image_size / self.steps[k]\n","\n","                # DBoxの中心座標 x,y　ただし、0～1で規格化している\n","                cx = (j + 0.5) / f_k\n","                cy = (i + 0.5) / f_k\n","\n","                # アスペクト比1の小さいDBox [cx,cy, width, height]\n","                # 'min_sizes': [30, 60, 111, 162, 213, 264]\n","                s_k = self.min_sizes[k]/self.image_size\n","                mean += [cx, cy, s_k, s_k]\n","\n","                # アスペクト比1の大きいDBox [cx,cy, width, height]\n","                # 'max_sizes': [60, 111, 162, 213, 264, 315],\n","                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n","                mean += [cx, cy, s_k_prime, s_k_prime]\n","\n","                # その他のアスペクト比のdefBox [cx,cy, width, height]\n","                for ar in self.aspect_ratios[k]:\n","                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n","                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n","\n","        # DBoxをテンソルに変換 torch.Size([8732, 4])\n","        output = torch.Tensor(mean).view(-1, 4)\n","\n","        # DBoxが画像の外にはみ出るのを防ぐため、大きさを最小0、最大1にする\n","        output.clamp_(max=1, min=0)\n","\n","        return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ep2s8MWl7ws7","colab_type":"code","colab":{}},"source":["# 動作の確認\n","\n","# SSD300の設定\n","ssd_cfg = {\n","    'num_classes': 21,  # 背景クラスを含めた合計クラス数\n","    'input_size': 300,  # 画像の入力サイズ\n","    'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\n","    'feature_maps': [38, 19, 10, 5, 3, 1],  # 各sourceの画像サイズ\n","    'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n","    'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\n","    'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\n","    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n","}\n","\n","# DBox作成\n","dbox = DBox(ssd_cfg)\n","dbox_list = dbox.make_dbox_list()\n","\n","# DBoxの出力を確認する\n","pd.DataFrame(dbox_list.numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uhVzQyDK719V","colab_type":"text"},"source":["# SSDクラスを実装"]},{"cell_type":"code","metadata":{"id":"vLx6PrQz7zJU","colab_type":"code","colab":{}},"source":["# SSDクラスを作成する\n","class SSD(nn.Module):\n","\n","    def __init__(self, phase, cfg):\n","        super(SSD, self).__init__()\n","\n","        self.phase = phase  # train or inferenceを指定\n","        self.num_classes = cfg[\"num_classes\"]  # クラス数=21\n","\n","        # SSDのネットワークを作る\n","        self.vgg = make_vgg()\n","        self.extras = make_extras()\n","        self.L2Norm = L2Norm()\n","        self.loc, self.conf = make_loc_conf(\n","            cfg[\"num_classes\"], cfg[\"bbox_aspect_num\"])\n","\n","        # DBox作成\n","        dbox = DBox(cfg)\n","        self.dbox_list = dbox.make_dbox_list()\n","\n","        # 推論時はクラス「Detect」を用意する\n","        if phase == 'inference':\n","            self.detect = Detect()\n","\n","\n","# 動作確認\n","ssd_test = SSD(phase=\"train\", cfg=ssd_cfg)\n","print(ssd_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ojTxW_vg782p","colab_type":"text"},"source":["# 関数decodeを実装する"]},{"cell_type":"code","metadata":{"id":"vv5UzdGK75gi","colab_type":"code","colab":{}},"source":["# オフセット情報を使い、DBoxをBBoxに変換する関数\n","\n","\n","def decode(loc, dbox_list):\n","    \"\"\"\n","    オフセット情報を使い、DBoxをBBoxに変換する。\n","\n","    Parameters\n","    ----------\n","    loc:  [8732,4]\n","        SSDモデルで推論するオフセット情報。\n","    dbox_list: [8732,4]\n","        DBoxの情報\n","\n","    Returns\n","    -------\n","    boxes : [xmin, ymin, xmax, ymax]\n","        BBoxの情報\n","    \"\"\"\n","\n","    # DBoxは[cx, cy, width, height]で格納されている\n","    # locも[Δcx, Δcy, Δwidth, Δheight]で格納されている\n","\n","    # オフセット情報からBBoxを求める\n","    boxes = torch.cat((\n","        dbox_list[:, :2] + loc[:, :2] * 0.1 * dbox_list[:, 2:],\n","        dbox_list[:, 2:] * torch.exp(loc[:, 2:] * 0.2)), dim=1)\n","    # boxesのサイズはtorch.Size([8732, 4])となります\n","\n","    # BBoxの座標情報を[cx, cy, width, height]から[xmin, ymin, xmax, ymax] に\n","    boxes[:, :2] -= boxes[:, 2:] / 2  # 座標(xmin,ymin)へ変換\n","    boxes[:, 2:] += boxes[:, :2]  # 座標(xmax,ymax)へ変換\n","\n","    return boxes"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rf3D5uiQ8EkU","colab_type":"text"},"source":["# Non-Maximum Suppressionを行う関数を実装する"]},{"cell_type":"code","metadata":{"id":"mGPkHrWg8B1E","colab_type":"code","colab":{}},"source":["# Non-Maximum Suppressionを行う関数\n","\n","\n","def nm_suppression(boxes, scores, overlap=0.45, top_k=200):\n","    \"\"\"\n","    Non-Maximum Suppressionを行う関数。\n","    boxesのうち被り過ぎ（overlap以上）のBBoxを削除する。\n","\n","    Parameters\n","    ----------\n","    boxes : [確信度閾値（0.01）を超えたBBox数,4]\n","        BBox情報。\n","    scores :[確信度閾値（0.01）を超えたBBox数]\n","        confの情報\n","\n","    Returns\n","    -------\n","    keep : リスト\n","        confの降順にnmsを通過したindexが格納\n","    count：int\n","        nmsを通過したBBoxの数\n","    \"\"\"\n","\n","    # returnのひな形を作成\n","    count = 0\n","    keep = scores.new(scores.size(0)).zero_().long()\n","    # keep：torch.Size([確信度閾値を超えたBBox数])、要素は全部0\n","\n","    # 各BBoxの面積areaを計算\n","    x1 = boxes[:, 0]\n","    y1 = boxes[:, 1]\n","    x2 = boxes[:, 2]\n","    y2 = boxes[:, 3]\n","    area = torch.mul(x2 - x1, y2 - y1)\n","\n","    # boxesをコピーする。後で、BBoxの被り度合いIOUの計算に使用する際のひな形として用意\n","    tmp_x1 = boxes.new()\n","    tmp_y1 = boxes.new()\n","    tmp_x2 = boxes.new()\n","    tmp_y2 = boxes.new()\n","    tmp_w = boxes.new()\n","    tmp_h = boxes.new()\n","\n","    # socreを昇順に並び変える\n","    v, idx = scores.sort(0)\n","\n","    # 上位top_k個（200個）のBBoxのindexを取り出す（200個存在しない場合もある）\n","    idx = idx[-top_k:]\n","\n","    # idxの要素数が0でない限りループする\n","    while idx.numel() > 0:\n","        i = idx[-1]  # 現在のconf最大のindexをiに\n","\n","        # keepの現在の最後にconf最大のindexを格納する\n","        # このindexのBBoxと被りが大きいBBoxをこれから消去する\n","        keep[count] = i\n","        count += 1\n","\n","        # 最後のBBoxになった場合は、ループを抜ける\n","        if idx.size(0) == 1:\n","            break\n","\n","        # 現在のconf最大のindexをkeepに格納したので、idxをひとつ減らす\n","        idx = idx[:-1]\n","\n","        # -------------------\n","        # これからkeepに格納したBBoxと被りの大きいBBoxを抽出して除去する\n","        # -------------------\n","        # ひとつ減らしたidxまでのBBoxを、outに指定した変数として作成する\n","        torch.index_select(x1, 0, idx, out=tmp_x1)\n","        torch.index_select(y1, 0, idx, out=tmp_y1)\n","        torch.index_select(x2, 0, idx, out=tmp_x2)\n","        torch.index_select(y2, 0, idx, out=tmp_y2)\n","\n","        # すべてのBBoxに対して、現在のBBox=indexがiと被っている値までに設定(clamp)\n","        tmp_x1 = torch.clamp(tmp_x1, min=x1[i])\n","        tmp_y1 = torch.clamp(tmp_y1, min=y1[i])\n","        tmp_x2 = torch.clamp(tmp_x2, max=x2[i])\n","        tmp_y2 = torch.clamp(tmp_y2, max=y2[i])\n","\n","        # wとhのテンソルサイズをindexを1つ減らしたものにする\n","        tmp_w.resize_as_(tmp_x2)\n","        tmp_h.resize_as_(tmp_y2)\n","\n","        # clampした状態でのBBoxの幅と高さを求める\n","        tmp_w = tmp_x2 - tmp_x1\n","        tmp_h = tmp_y2 - tmp_y1\n","\n","        # 幅や高さが負になっているものは0にする\n","        tmp_w = torch.clamp(tmp_w, min=0.0)\n","        tmp_h = torch.clamp(tmp_h, min=0.0)\n","\n","        # clampされた状態での面積を求める\n","        inter = tmp_w*tmp_h\n","\n","        # IoU = intersect部分 / (area(a) + area(b) - intersect部分)の計算\n","        rem_areas = torch.index_select(area, 0, idx)  # 各BBoxの元の面積\n","        union = (rem_areas - inter) + area[i]  # 2つのエリアのANDの面積\n","        IoU = inter/union\n","\n","        # IoUがoverlapより小さいidxのみを残す\n","        idx = idx[IoU.le(overlap)]  # leはLess than or Equal toの処理をする演算\n","        # IoUがoverlapより大きいidxは、最初に選んでkeepに格納したidxと同じ物体に対してBBoxを囲んでいるため消去\n","\n","    # whileのループが抜けたら終了\n","\n","    return keep, count\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hRqXsINo8NLQ","colab_type":"text"},"source":["# Detectクラスを実装する"]},{"cell_type":"code","metadata":{"id":"E-GohKPF8K88","colab_type":"code","colab":{}},"source":["# SSDの推論時にconfとlocの出力から、被りを除去したBBoxを出力する\n","\n","\n","class Detect(Function):\n","\n","    def __init__(self, conf_thresh=0.01, top_k=200, nms_thresh=0.45):\n","        self.softmax = nn.Softmax(dim=-1)  # confをソフトマックス関数で正規化するために用意\n","        self.conf_thresh = conf_thresh  # confがconf_thresh=0.01より高いDBoxのみを扱う\n","        self.top_k = top_k  # nm_supressionでconfの高いtop_k個を計算に使用する, top_k = 200\n","        self.nms_thresh = nms_thresh  # nm_supressionでIOUがnms_thresh=0.45より大きいと、同一物体へのBBoxとみなす\n","\n","    def forward(self, loc_data, conf_data, dbox_list):\n","        \"\"\"\n","        順伝搬の計算を実行する。\n","\n","        Parameters\n","        ----------\n","        loc_data:  [batch_num,8732,4]\n","            オフセット情報。\n","        conf_data: [batch_num, 8732,num_classes]\n","            検出の確信度。\n","        dbox_list: [8732,4]\n","            DBoxの情報\n","\n","        Returns\n","        -------\n","        output : torch.Size([batch_num, 21, 200, 5])\n","            （batch_num、クラス、confのtop200、BBoxの情報）\n","        \"\"\"\n","\n","        # 各サイズを取得\n","        num_batch = loc_data.size(0)  # ミニバッチのサイズ\n","        num_dbox = loc_data.size(1)  # DBoxの数 = 8732\n","        num_classes = conf_data.size(2)  # クラス数 = 21\n","\n","        # confはソフトマックスを適用して正規化する\n","        conf_data = self.softmax(conf_data)\n","\n","        # 出力の型を作成する。テンソルサイズは[minibatch数, 21, 200, 5]\n","        output = torch.zeros(num_batch, num_classes, self.top_k, 5)\n","\n","        # cof_dataを[batch_num,8732,num_classes]から[batch_num, num_classes,8732]に順番変更\n","        conf_preds = conf_data.transpose(2, 1)\n","\n","        # ミニバッチごとのループ\n","        for i in range(num_batch):\n","\n","            # 1. locとDBoxから修正したBBox [xmin, ymin, xmax, ymax] を求める\n","            decoded_boxes = decode(loc_data[i], dbox_list)\n","\n","            # confのコピーを作成\n","            conf_scores = conf_preds[i].clone()\n","\n","            # 画像クラスごとのループ（背景クラスのindexである0は計算せず、index=1から）\n","            for cl in range(1, num_classes):\n","\n","                # 2.confの閾値を超えたBBoxを取り出す\n","                # confの閾値を超えているかのマスクを作成し、\n","                # 閾値を超えたconfのインデックスをc_maskとして取得\n","                c_mask = conf_scores[cl].gt(self.conf_thresh)\n","                # gtはGreater thanのこと。gtにより閾値を超えたものが1に、以下が0になる\n","                # conf_scores:torch.Size([21, 8732])\n","                # c_mask:torch.Size([8732])\n","\n","                # scoresはtorch.Size([閾値を超えたBBox数])\n","                scores = conf_scores[cl][c_mask]\n","\n","                # 閾値を超えたconfがない場合、つまりscores=[]のときは、何もしない\n","                if scores.nelement() == 0:  # nelementで要素数の合計を求める\n","                    continue\n","\n","                # c_maskを、decoded_boxesに適用できるようにサイズを変更します\n","                l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes)\n","                # l_mask:torch.Size([8732, 4])\n","\n","                # l_maskをdecoded_boxesに適応します\n","                boxes = decoded_boxes[l_mask].view(-1, 4)\n","                # decoded_boxes[l_mask]で1次元になってしまうので、\n","                # viewで（閾値を超えたBBox数, 4）サイズに変形しなおす\n","\n","                # 3. Non-Maximum Suppressionを実施し、被っているBBoxを取り除く\n","                ids, count = nm_suppression(\n","                    boxes, scores, self.nms_thresh, self.top_k)\n","                # ids：confの降順にNon-Maximum Suppressionを通過したindexが格納\n","                # count：Non-Maximum Suppressionを通過したBBoxの数\n","\n","                # outputにNon-Maximum Suppressionを抜けた結果を格納\n","                output[i, cl, :count] = torch.cat((scores[ids[:count]].unsqueeze(1),\n","                                                   boxes[ids[:count]]), 1)\n","\n","        return output  # torch.Size([1, 21, 200, 5])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"efxhq9fS8Vrh","colab_type":"text"},"source":["# SSDクラスを実装する"]},{"cell_type":"code","metadata":{"id":"cjRx7u5Y8Tx9","colab_type":"code","colab":{}},"source":["# SSDクラスを作成する\n","\n","class SSD(nn.Module):\n","\n","    def __init__(self, phase, cfg):\n","        super(SSD, self).__init__()\n","\n","        self.phase = phase  # train or inferenceを指定\n","        self.num_classes = cfg[\"num_classes\"]  # クラス数=21\n","\n","        # SSDのネットワークを作る\n","        self.vgg = make_vgg()\n","        self.extras = make_extras()\n","        self.L2Norm = L2Norm()\n","        self.loc, self.conf = make_loc_conf(\n","            cfg[\"num_classes\"], cfg[\"bbox_aspect_num\"])\n","\n","        # DBox作成\n","        dbox = DBox(cfg)\n","        self.dbox_list = dbox.make_dbox_list()\n","\n","        # 推論時はクラス「Detect」を用意する\n","        if phase == 'inference':\n","            self.detect = Detect()\n","\n","    def forward(self, x):\n","        sources = list()  # locとconfへの入力source1～6を格納\n","        loc = list()  # locの出力を格納\n","        conf = list()  # confの出力を格納\n","\n","        # vggのconv4_3まで計算する\n","        for k in range(23):\n","            x = self.vgg[k](x)\n","\n","        # conv4_3の出力をL2Normに入力し、source1を作成、sourcesに追加\n","        source1 = self.L2Norm(x)\n","        sources.append(source1)\n","\n","        # vggを最後まで計算し、source2を作成、sourcesに追加\n","        for k in range(23, len(self.vgg)):\n","            x = self.vgg[k](x)\n","\n","        sources.append(x)\n","\n","        # extrasのconvとReLUを計算\n","        # source3～6を、sourcesに追加\n","        for k, v in enumerate(self.extras):\n","            x = F.relu(v(x), inplace=True)\n","            if k % 2 == 1:  # conv→ReLU→cov→ReLUをしたらsourceに入れる\n","                sources.append(x)\n","\n","        # source1～6に、それぞれ対応する畳み込みを1回ずつ適用する\n","        # zipでforループの複数のリストの要素を取得\n","        # source1～6まであるので、6回ループが回る\n","        for (x, l, c) in zip(sources, self.loc, self.conf):\n","            # Permuteは要素の順番を入れ替え\n","            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n","            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n","            # l(x)とc(x)で畳み込みを実行\n","            # l(x)とc(x)の出力サイズは[batch_num, 4*アスペクト比の種類数, featuremapの高さ, featuremap幅]\n","            # sourceによって、アスペクト比の種類数が異なり、面倒なので順番入れ替えて整える\n","            # permuteで要素の順番を入れ替え、\n","            # [minibatch数, featuremap数, featuremap数,4*アスペクト比の種類数]へ\n","            # （注釈）\n","            # torch.contiguous()はメモリ上で要素を連続的に配置し直す命令です。\n","            # あとでview関数を使用します。\n","            # このviewを行うためには、対象の変数がメモリ上で連続配置されている必要があります。\n","\n","        # さらにlocとconfの形を変形\n","        # locのサイズは、torch.Size([batch_num, 34928])\n","        # confのサイズはtorch.Size([batch_num, 183372])になる\n","        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n","        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n","\n","        # さらにlocとconfの形を整える\n","        # locのサイズは、torch.Size([batch_num, 8732, 4])\n","        # confのサイズは、torch.Size([batch_num, 8732, 21])\n","        loc = loc.view(loc.size(0), -1, 4)\n","        conf = conf.view(conf.size(0), -1, self.num_classes)\n","\n","        # 最後に出力する\n","        output = (loc, conf, self.dbox_list)\n","\n","        if self.phase == \"inference\":  # 推論時\n","            # クラス「Detect」のforwardを実行\n","            # 返り値のサイズは torch.Size([batch_num, 21, 200, 5])\n","            return self.detect(output[0], output[1], output[2])\n","\n","        else:  # 学習時\n","            return output\n","            # 返り値は(loc, conf, dbox_list)のタプル\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FNAErUfe-yXD","colab_type":"text"},"source":["# 学習"]},{"cell_type":"code","metadata":{"id":"fPC-9zHX-xnq","colab_type":"code","colab":{}},"source":["# パッケージのimport\n","import os.path as osp\n","import random\n","import time\n","\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","import torch.optim as optim\n","import torch.utils.data as data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HInzhloV8bl8","colab_type":"code","colab":{}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"使用デバイス：\", device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4T4mZiSa8oTf","colab_type":"text"},"source":["# DatasetとDataLoaderを作成する"]},{"cell_type":"code","metadata":{"id":"sJFZsSDU8o8n","colab_type":"code","colab":{}},"source":["from utils.ssd_model import make_datapath_list, VOCDataset, DataTransform, Anno_xml2list, od_collate_fn\n","\n","\n","# ファイルパスのリストを取得\n","rootpath = \"./data/VOCdevkit/VOC2012/\"\n","train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\n","    rootpath)\n","\n","# Datasetを作成\n","voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n","               'bottle', 'bus', 'car', 'cat', 'chair',\n","               'cow', 'diningtable', 'dog', 'horse',\n","               'motorbike', 'person', 'pottedplant',\n","               'sheep', 'sofa', 'train', 'tvmonitor']\n","color_mean = (104, 117, 123)  # (BGR)の色の平均値\n","input_size = 300  # 画像のinputサイズを300×300にする\n","\n","train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", transform=DataTransform(\n","    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n","\n","val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(\n","    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n","\n","# DataLoaderを作成する\n","batch_size = 32\n","\n","train_dataloader = data.DataLoader(\n","    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn)\n","\n","val_dataloader = data.DataLoader(\n","    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn)\n","\n","# 辞書オブジェクトにまとめる\n","dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k8VrFGbu-KQI","colab_type":"text"},"source":["# ネットワークモデルの作成する"]},{"cell_type":"code","metadata":{"id":"_ehK-pDJ8yUK","colab_type":"code","colab":{}},"source":["from utils.ssd_model import SSD\n","\n","# SSD300の設定\n","ssd_cfg = {\n","    'num_classes': 21,  # 背景クラスを含めた合計クラス数\n","    'input_size': 300,  # 画像の入力サイズ\n","    'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\n","    'feature_maps': [38, 19, 10, 5, 3, 1],  # 各sourceの画像サイズ\n","    'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n","    'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\n","    'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\n","    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n","}\n","\n","# SSDネットワークモデル\n","net = SSD(phase=\"train\", cfg=ssd_cfg)\n","\n","# SSDの初期の重みを設定\n","# ssdのvgg部分に重みをロードする\n","vgg_weights = torch.load('./weights/vgg16_reducedfc.pth')\n","net.vgg.load_state_dict(vgg_weights)\n","\n","# ssdのその他のネットワークの重みはHeの初期値で初期化\n","def weights_init(m):\n","    if isinstance(m, nn.Conv2d):\n","        init.kaiming_normal_(m.weight.data)\n","        if m.bias is not None:  # バイアス項がある場合\n","            nn.init.constant_(m.bias, 0.0)\n","\n","\n","# Heの初期値を適用\n","net.extras.apply(weights_init)\n","net.loc.apply(weights_init)\n","net.conf.apply(weights_init)\n","\n","# GPUが使えるかを確認\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"使用デバイス：\", device)\n","\n","print('ネットワーク設定完了：学習済みの重みをロードしました')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F-0hV4-J-Rha","colab_type":"text"},"source":["# 損失関数と最適化手法を定義する"]},{"cell_type":"code","metadata":{"id":"sGnGxwqM-PMU","colab_type":"code","colab":{}},"source":["from utils.ssd_model import MultiBoxLoss\n","\n","# 損失関数の設定\n","criterion = MultiBoxLoss(jaccard_thresh=0.5, neg_pos=3, device=device)\n","\n","# 最適化手法の設定\n","optimizer = optim.SGD(net.parameters(), lr=1e-3,\n","                      momentum=0.9, weight_decay=5e-4)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Op9-OZAm-bpt","colab_type":"text"},"source":["# 学習・検証を実施する"]},{"cell_type":"code","metadata":{"id":"_lsO1bdQ-WA7","colab_type":"code","colab":{}},"source":["# モデルを学習させる関数を作成\n","def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n","\n","    # GPUが使えるかを確認\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    print(\"使用デバイス：\", device)\n","\n","    # ネットワークをGPUへ\n","    net.to(device)\n","\n","    # ネットワークがある程度固定であれば、高速化させる\n","    torch.backends.cudnn.benchmark = True\n","\n","    # イテレーションカウンタをセット\n","    iteration = 1\n","    epoch_train_loss = 0.0  # epochの損失和\n","    epoch_val_loss = 0.0  # epochの損失和\n","    logs = []\n","\n","    # epochのループ\n","    for epoch in range(num_epochs+1):\n","\n","        # 開始時刻を保存\n","        t_epoch_start = time.time()\n","        t_iter_start = time.time()\n","\n","        print('-------------')\n","        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n","        print('-------------')\n","\n","        # epochごとの訓練と検証のループ\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                net.train()  # モデルを訓練モードに\n","                print('（train）')\n","            else:\n","                if((epoch+1) % 10 == 0):\n","                    net.eval()   # モデルを検証モードに\n","                    print('-------------')\n","                    print('（val）')\n","                else:\n","                    # 検証は10回に1回だけ行う\n","                    continue\n","\n","            # データローダーからminibatchずつ取り出すループ\n","            for images, targets in dataloaders_dict[phase]:\n","\n","                # GPUが使えるならGPUにデータを送る\n","                images = images.to(device)\n","                targets = [ann.to(device)\n","                           for ann in targets]  # リストの各要素のテンソルをGPUへ\n","\n","                # optimizerを初期化\n","                optimizer.zero_grad()\n","\n","                # 順伝搬（forward）計算\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    # 順伝搬（forward）計算\n","                    outputs = net(images)\n","\n","                    # 損失の計算\n","                    loss_l, loss_c = criterion(outputs, targets)\n","                    loss = loss_l + loss_c\n","\n","                    # 訓練時はバックプロパゲーション\n","                    if phase == 'train':\n","                        loss.backward()  # 勾配の計算\n","\n","                        # 勾配が大きくなりすぎると計算が不安定になるので、clipで最大でも勾配2.0に留める\n","                        nn.utils.clip_grad_value_(\n","                            net.parameters(), clip_value=2.0)\n","\n","                        optimizer.step()  # パラメータ更新\n","\n","                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n","                            t_iter_finish = time.time()\n","                            duration = t_iter_finish - t_iter_start\n","                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n","                                iteration, loss.item(), duration))\n","                            t_iter_start = time.time()\n","\n","                        epoch_train_loss += loss.item()\n","                        iteration += 1\n","\n","                    # 検証時\n","                    else:\n","                        epoch_val_loss += loss.item()\n","\n","        # epochのphaseごとのlossと正解率\n","        t_epoch_finish = time.time()\n","        print('-------------')\n","        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n","            epoch+1, epoch_train_loss, epoch_val_loss))\n","        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n","        t_epoch_start = time.time()\n","\n","        # ログを保存\n","        log_epoch = {'epoch': epoch+1,\n","                     'train_loss': epoch_train_loss, 'val_loss': epoch_val_loss}\n","        logs.append(log_epoch)\n","        df = pd.DataFrame(logs)\n","        df.to_csv(\"log_output.csv\")\n","\n","        epoch_train_loss = 0.0  # epochの損失和\n","        epoch_val_loss = 0.0  # epochの損失和\n","\n","        # ネットワークを保存する\n","        if ((epoch+1) % 10 == 0):\n","            torch.save(net.state_dict(), 'weights/ssd300_' +\n","                       str(epoch+1) + '.pth')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qouxEA4n-htZ","colab_type":"code","colab":{}},"source":["# 学習・検証を実行する\n","num_epochs= 50  \n","train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"glLXDeztAldm","colab_type":"text"},"source":["# 推論"]},{"cell_type":"code","metadata":{"id":"b4Ef8E4uA8WC","colab_type":"code","colab":{}},"source":["import cv2  # OpenCVライブラリ\n","import matplotlib.pyplot as plt \n","import numpy as np\n","import torch\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UnukWvav-jqP","colab_type":"code","colab":{}},"source":["from utils.ssd_model import SSD\n","\n","voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n","               'bottle', 'bus', 'car', 'cat', 'chair',\n","               'cow', 'diningtable', 'dog', 'horse',\n","               'motorbike', 'person', 'pottedplant',\n","               'sheep', 'sofa', 'train', 'tvmonitor']\n","\n","# SSD300の設定\n","ssd_cfg = {\n","    'num_classes': 21,  # 背景クラスを含めた合計クラス数\n","    'input_size': 300,  # 画像の入力サイズ\n","    'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\n","    'feature_maps': [38, 19, 10, 5, 3, 1],  # 各sourceの画像サイズ\n","    'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n","    'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\n","    'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\n","    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n","}\n","\n","# SSDネットワークモデル\n","net = SSD(phase=\"inference\", cfg=ssd_cfg)\n","\n","# SSDの学習済みの重みを設定\n","# net_weights = torch.load('./weights/ssd300_50.pth',\n","#                          map_location={'cuda:0': 'cpu'})\n","\n","net_weights = torch.load('./weights/ssd300_mAP_77.43_v2.pth',\n","                        map_location={'cuda:0': 'cpu'})\n","\n","net.load_state_dict(net_weights)\n","\n","print('ネットワーク設定完了：学習済みの重みをロードしました')\n","\n","import cv2  # OpenCVライブラリ\n","import matplotlib.pyplot as plt \n","import numpy as np\n","import torch\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rOe1QhwzAqw9","colab_type":"code","colab":{}},"source":["\n","from utils.ssd_model import DataTransform\n","\n","# 1. 画像読み込み\n","image_file_path = \"./data/cowboy-757575_640.jpg\"\n","img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n","height, width, channels = img.shape  # 画像のサイズを取得\n","\n","# 2. 元画像の表示\n","plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","plt.show()\n","\n","# 3. 前処理クラスの作成\n","color_mean = (104, 117, 123)  # (BGR)の色の平均値\n","input_size = 300  # 画像のinputサイズを300×300にする\n","transform = DataTransform(input_size, color_mean)\n","\n","# 4. 前処理\n","phase = \"val\"\n","img_transformed, boxes, labels = transform(\n","    img, phase, \"\", \"\")  # アノテーションはないので、\"\"にする\n","img = torch.from_numpy(img_transformed[:, :, (2, 1, 0)]).permute(2, 0, 1)\n","\n","# 5. SSDで予測\n","net.eval()  # ネットワークを推論モードへ\n","x = img.unsqueeze(0)  # ミニバッチ化：torch.Size([1, 3, 300, 300])\n","detections = net(x)\n","\n","print(detections.shape)\n","print(detections)\n","\n","# output : torch.Size([batch_num, 21, 200, 5])\n","#  =（batch_num、クラス、confのtop200、規格化されたBBoxの情報）\n","#   規格化されたBBoxの情報（確信度、xmin, ymin, xmax, ymax）\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"47OiQYMVAxGh","colab_type":"code","colab":{}},"source":["# 画像に対する予測\n","from utils.ssd_predict_show import SSDPredictShow\n","\n","# ファイルパス\n","image_file_path = \"./data/cowboy-757575_640.jpg\"\n","\n","# 予測と、予測結果を画像で描画する\n","ssd = SSDPredictShow(eval_categories=voc_classes, net=net)\n","ssd.show(image_file_path, data_confidence_level=0.6)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xtpIKB_VBoqP","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}