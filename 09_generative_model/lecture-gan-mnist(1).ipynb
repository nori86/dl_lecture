{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lecture-gan-mnist.ipynb","version":"0.3.2","provenance":[{"file_id":"1bxfG7-qT31OliIv7arLYZ8TERLHxfpTo","timestamp":1541393055669}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"HXI28510PYej","colab_type":"text"},"cell_type":"markdown","source":["# GAN-MNIST"]},{"metadata":{"id":"icBwq1iPPZv2","colab_type":"code","outputId":"44f85ebf-b189-41e4-bb4b-dbd45d9dfa28","executionInfo":{"status":"ok","timestamp":1541393124486,"user_tz":-540,"elapsed":2442,"user":{"displayName":"nori 86","photoUrl":"","userId":"17990641330801160498"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["from keras.datasets import mnist\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"kcSWM527OI8l","colab_type":"code","colab":{}},"cell_type":"code","source":["def load_mnist(dim=3, data='mnist'):\n","    img_rows, img_cols = 28, 28\n","    \n","    (train_X, train_y), (test_X, test_y) = mnist.load_data()\n","    \n","    train_X = train_X.reshape(train_X.shape[0], img_rows, img_cols, 1)\n","    test_X = test_X.reshape(test_X.shape[0], img_rows, img_cols, 1)\n","        \n","    train_X = train_X.astype('float32') / 255\n","    test_X = test_X.astype('float32') / 255\n","    train_y = np.eye(10)[train_y]\n","    test_y = np.eye(10)[test_y]\n","    \n","    return  train_X, test_X, train_y, test_y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BQq8L5iaOTP8","colab_type":"code","colab":{}},"cell_type":"code","source":["def plot_mnist(n_ex=10,dim=(2,5), figsize=(8,4)):\n","    noise = np.random.uniform(0,1,size=[n_ex,100])\n","    generated_images = generator.predict(noise)\n","\n","    plt.figure(figsize=figsize)\n","    for i in range(generated_images.shape[0]):\n","        plt.subplot(dim[0],dim[1],i+1)\n","        img = generated_images[i,:,:, 0]\n","        plt.imshow(img, cmap='binary')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tZaJN-M1OhUV","colab_type":"text"},"cell_type":"markdown","source":["## Generator"]},{"metadata":{"id":"WMmu-ce6OUHE","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.layers import Input\n","from keras.layers.core import Reshape, Dense, Flatten, Activation\n","from keras.layers.convolutional import Conv2D, UpSampling2D\n","from keras.layers.normalization import BatchNormalization\n","from keras.models import Model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"n0hMYJhxR227","colab_type":"code","colab":{}},"cell_type":"code","source":["def Generator():\n","    nch = 200\n","    model_input = Input(shape=[100])\n","    x = Dense(nch*14*14, \n","              kernel_initializer='glorot_normal')(model_input) # 100 -> 200*14*14\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = Reshape( [14, 14, nch] )(x) # 200*14*14 -> 14x14x200 (width)x(height)x(channel)\n","    x = UpSampling2D(size=(2, 2))(x) # 14x14x200 -> 28x28x200\n","    x = Conv2D(int(nch/2), (3, 3), padding='same', \n","               kernel_initializer='glorot_uniform')(x) # 28x28x200 -> 28x28x100\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = Conv2D(int(nch/4), (3, 3), padding='same', \n","               kernel_initializer='glorot_uniform')(x) # 28x28x100 -> 28x28x50\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = Conv2D(1, (1, 1), padding='same', \n","               kernel_initializer='glorot_uniform')(x) # 28x28x50 -> 28x28x1\n","    model_output = Activation('sigmoid')(x)\n","    model = Model(model_input, model_output)\n","    # model.summary()\n","    \n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QDmro54eOnU0","colab_type":"text"},"cell_type":"markdown","source":["## Discriminator"]},{"metadata":{"id":"00DqB52vRpwP","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.layers import Input\n","from keras.layers.core import Reshape, Dense, Dropout, Flatten\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import Conv2D\n","from keras.layers.normalization import BatchNormalization\n","from keras.optimizers import Adam\n","from keras.models import Model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UwRHy1diOlJP","colab_type":"code","colab":{}},"cell_type":"code","source":["def Discriminator(shape, dropout_rate=0.25, opt=Adam(lr=1e-4)):\n","    model_input = Input(shape=shape) # 28x28x1\n","    x = Conv2D(256, (5, 5), padding = 'same', \n","               kernel_initializer='glorot_uniform', \n","               strides=(2, 2))(model_input) # 28x28x1 -> 14x14x256\n","    x = LeakyReLU(0.2)(x)\n","    x = Dropout(dropout_rate)(x)\n","    x = Conv2D(512, (5, 5), padding = 'same', \n","               kernel_initializer='glorot_uniform', \n","               strides=(2, 2))(x) # 14x14x256 -> 7x7x512\n","    x = LeakyReLU(0.2)(x)\n","    x = Dropout(dropout_rate)(x)\n","    x = Flatten()(x) # 7x7x512 -> 7*7*512\n","    x = Dense(256)(x)\n","    x = LeakyReLU(0.2)(x)\n","    x = Dropout(dropout_rate)(x)\n","    model_output = Dense(2,activation='softmax')(x) # 7*7*512 -> 2\n","    model = Model(model_input, model_output)\n","    model.compile(loss='categorical_crossentropy', optimizer=opt)\n","    # model.summary()\n","    \n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0UMvlhXNOu8a","colab_type":"text"},"cell_type":"markdown","source":["## GANの学習"]},{"metadata":{"id":"7A2z5Z-EOqCX","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.layers import Input\n","from keras.models import Model\n","\n","def combined_network(generator, discriminator, opt=Adam(lr=1e-3)):\n","    gan_input = Input(shape=[100])\n","    x = generator(gan_input)\n","    gan_output = discriminator(x)\n","    model = Model(gan_input, gan_output)\n","    model.compile(loss='categorical_crossentropy', optimizer=opt)\n","    # model.summary()\n","    \n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yvhrb6-3Oxsi","colab_type":"code","colab":{}},"cell_type":"code","source":["def make_trainable(net, val):\n","    net.trainable = val\n","    for l in net.layers:\n","        l.trainable = val"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K4RMEUP7O6VB","colab_type":"code","colab":{}},"cell_type":"code","source":["from tqdm import tqdm\n","\n","def train(step=3000, BATCH_SIZE=128):\n","    for e in tqdm(range(step)):\n","        # 1. バッチの学習で利用する画像の選択 \n","        # バッチサイズの分だけランダムに画像を選択\n","        image_batch = X_train[np.random.randint(0,X_train.shape[0],size=BATCH_SIZE),:,:,:]\n","        \n","        # バッチサイズの分だけランダムにノイズを生成し、generatorにより画像を生成\n","        noise_gen = np.random.uniform(0,1,size=[BATCH_SIZE,100])\n","        generated_images = generator.predict(noise_gen)\n","        \n","        # 2. Discriminatorの学習をonに切り替える\n","        # Discriminatorが学習するように変更\n","        make_trainable(discriminator,True)\n","        \n","        # 3. Generatorによる生成画像を用いてDiscriminatorの学習\n","        # X = (バッチサイズ分のデータセットの画像, バッチサイズ分の生成画像)\n","        X = np.concatenate((image_batch, generated_images))\n","        \n","        # y = (バッチサイズ分のTrue(本物), バッチサイズ分のFalse(偽物))\n","        y = np.zeros([2*BATCH_SIZE,2])\n","        y[:BATCH_SIZE,1] = 1\n","        y[BATCH_SIZE:,0] = 1      \n","        \n","        # Discriminatorのtrain\n","        discriminator.train_on_batch(X,y)\n","        \n","        # 4. Discriminatorの学習をoffに切り替える\n","        # Discriminatorが学習しないように変更\n","        make_trainable(discriminator,False)\n","    \n","        # 5. Generatorの学習\n","        # バッチサイズの分だけランダムにノイズを生成\n","        noise_gen = np.random.uniform(0,1,size=[BATCH_SIZE,100])\n","        \n","        # y = (バッチサイズ分のTrue(本物))\n","        # 実際には生成した画像なのでDiscriminatorとしては偽物と判断すべきだが、\n","        # Genaratorの学習なので生成した画像を本物と判断するように学習させる\n","        y2 = np.zeros([BATCH_SIZE,2])\n","        y2[:,1] = 1\n","        \n","        # Generatorのtrain\n","        GAN.train_on_batch(noise_gen, y2 )"],"execution_count":0,"outputs":[]},{"metadata":{"id":"F8Pgzr2VO_kM","colab_type":"text"},"cell_type":"markdown","source":["## MNISTの学習"]},{"metadata":{"id":"8k-PwVA-O9Uv","colab_type":"code","outputId":"1fe909b9-683e-4789-de4c-2ba335c945dd","executionInfo":{"status":"ok","timestamp":1541393148238,"user_tz":-540,"elapsed":7722,"user":{"displayName":"nori 86","photoUrl":"","userId":"17990641330801160498"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"cell_type":"code","source":["# データのロード\n","X_train, _,_,_ = load_mnist()\n","# それぞれのネットワークのインスタンスを生成\n","generator = Generator()\n","discriminator = Discriminator(X_train.shape[1:])\n","make_trainable(discriminator, False)\n","GAN = combined_network(generator, discriminator) "],"execution_count":11,"outputs":[{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n","11493376/11490434 [==============================] - 2s 0us/step\n"],"name":"stdout"}]},{"metadata":{"id":"YsH2436HPEIN","colab_type":"code","outputId":"e4d339c8-53f7-4461-d5fb-f10a5ff4cdc9","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1541394403273,"user_tz":-540,"elapsed":1254914,"user":{"displayName":"nori 86","photoUrl":"","userId":"17990641330801160498"}}},"cell_type":"code","source":["train()"],"execution_count":12,"outputs":[{"output_type":"stream","text":["100%|██████████| 3000/3000 [20:52<00:00,  2.42it/s]\n"],"name":"stderr"}]},{"metadata":{"id":"42UnLI-xJYUj","colab_type":"code","colab":{}},"cell_type":"code","source":["plot_mnist()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AayjoTCfPIik","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}